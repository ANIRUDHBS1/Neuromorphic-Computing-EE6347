{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "**EE6347 : Devices and Technologies for AI and Neuromorphic Computing**\n",
        "\n",
        "**Assignment 7 : Image Classification using Spiking Neural Network**\n",
        "\n",
        "Name : ANIRUDH B S ; Roll No. : EE21B019\n",
        "</center>"
      ],
      "metadata": {
        "id": "Qnepouv8hQjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to install the required library : [snntorch](https://snntorch.readthedocs.io/en/latest/)"
      ],
      "metadata": {
        "id": "aRbqGR08gBAH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WI4qJc-bMVd",
        "outputId": "55335f0c-46a0-4bd6-ed02-40dc40ca0030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.12.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (3.0.2)\n",
            "Downloading snntorch-0.9.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nir-1.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.4 nirtorch-1.0 snntorch-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will import all the required modules from snntorch."
      ],
      "metadata": {
        "id": "ZTXoyjZVgP5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJnYPDEQ4OoH",
        "outputId": "28219c69-e9ba-4831-e247-7e1a32405be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-159e52679525>:3: DeprecationWarning: The module snntorch.backprop will be deprecated in  a future release. Writing out your own training loop will lead to substantially faster performance.\n",
            "  from snntorch import backprop\n"
          ]
        }
      ],
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import backprop\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZI1YAJnQcXK"
      },
      "source": [
        "Load the EMNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI_DS_7FNlO8",
        "outputId": "f0d74a9e-9f74-48ae-95d8-3b479acc2b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:09<00:00, 60.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n"
          ]
        }
      ],
      "source": [
        "# Define transform to preprocess the data\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "# Loading the 'balanced' split of EMNIST dataset\n",
        "emnist_dataset_train = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "emnist_dataset_test = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Splitting into train and validation sets\n",
        "train_val_split = 0.9\n",
        "train_size = int(train_val_split * len(emnist_dataset_train))\n",
        "val_size = len(emnist_dataset_train) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(emnist_dataset_train, [train_size, val_size])\n",
        "\n",
        "# Creating data loaders\n",
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
        "test_loader = torch.utils.data.DataLoader(emnist_dataset_test, batch_size=batch_size, shuffle=True, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q50AZ0IDOotB",
        "outputId": "9c0f0b52-8de9-49a5-cbc2-39a11156c440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([5, 128, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Neuron and simulation parameters\n",
        "#spike_grad = surrogate.fast_sigmoid(slope=25)\n",
        "spike_grad = surrogate.atan(alpha=2.0)\n",
        "beta = 0.5\n",
        "num_steps = 5\n",
        "\n",
        "# Defining the network architecture\n",
        "# Initialize Network\n",
        "net = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(12, 64, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(64*4*4, 47),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)\n",
        "\n",
        "data, targets = next(iter(train_loader))\n",
        "#spike_data = spikegen.delta(data, threshold=0.1, padding=False, off_spike=False)\n",
        "#spike_data = spikegen.latency(data, num_steps=num_steps, normalize=True, linear=True)\n",
        "spike_data = spikegen.rate(data, num_steps=num_steps)\n",
        "spike_data = spike_data.to(device)\n",
        "print(data.size())\n",
        "print(spike_data.size())\n",
        "targets = targets.to(device)\n",
        "\n",
        "for step in range(num_steps):\n",
        "    spk_out, mem_out = net(data)\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(net, num_steps, spike_data):\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps):\n",
        "      spk_out, mem_out = net(spike_data[step])\n",
        "      spk_rec.append(spk_out)\n",
        "      mem_rec.append(mem_out)\n",
        "\n",
        "  return torch.stack(spk_rec), torch.stack(mem_rec)\n",
        "\n",
        "#spk_rec, mem_rec = forward_pass(net, num_steps, spike_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KtMbtKM7WEs8"
      },
      "outputs": [],
      "source": [
        "loss_fn = SF.mse_count_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sIEAEAhVQttF"
      },
      "outputs": [],
      "source": [
        "# Defining the function for accuracy calculation\n",
        "def batch_accuracy(data_loader, net, num_steps):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      spike_data = spikegen.rate(data, num_steps=num_steps)\n",
        "      spike_data = spike_data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      spk_rec, _ = forward_pass(net, num_steps, spike_data)\n",
        "\n",
        "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX1FK3cxRRP1"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SmoJSztzRNwj",
        "outputId": "1aabd26a-7729-485a-8145-90eeeecdd3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Val Acc: 2.11%\n",
            "\n",
            "Iteration 100, Val Acc: 62.68%\n",
            "\n",
            "Iteration 200, Val Acc: 70.87%\n",
            "\n",
            "Iteration 300, Val Acc: 74.87%\n",
            "\n",
            "Iteration 400, Val Acc: 75.47%\n",
            "\n",
            "Iteration 500, Val Acc: 76.30%\n",
            "\n",
            "Iteration 600, Val Acc: 78.38%\n",
            "\n",
            "Iteration 700, Val Acc: 78.97%\n",
            "\n",
            "Iteration 800, Val Acc: 79.17%\n",
            "\n",
            "Iteration 900, Val Acc: 79.15%\n",
            "\n",
            "Iteration 1000, Val Acc: 79.83%\n",
            "\n",
            "Iteration 1100, Val Acc: 79.87%\n",
            "\n",
            "Iteration 1200, Val Acc: 80.56%\n",
            "\n",
            "Iteration 1300, Val Acc: 81.11%\n",
            "\n",
            "Iteration 1400, Val Acc: 81.12%\n",
            "\n",
            "Iteration 1500, Val Acc: 80.98%\n",
            "\n",
            "Iteration 1600, Val Acc: 80.93%\n",
            "\n",
            "Iteration 1700, Val Acc: 81.84%\n",
            "\n",
            "Iteration 1800, Val Acc: 81.41%\n",
            "\n",
            "Iteration 1900, Val Acc: 81.82%\n",
            "\n",
            "Iteration 2000, Val Acc: 81.99%\n",
            "\n",
            "Iteration 2100, Val Acc: 81.26%\n",
            "\n",
            "Iteration 2200, Val Acc: 82.55%\n",
            "\n",
            "Iteration 2300, Val Acc: 81.49%\n",
            "\n",
            "Iteration 2400, Val Acc: 81.78%\n",
            "\n",
            "Iteration 2500, Val Acc: 81.76%\n",
            "\n",
            "Iteration 2600, Val Acc: 82.20%\n",
            "\n",
            "Iteration 2700, Val Acc: 81.90%\n",
            "\n",
            "Iteration 2800, Val Acc: 82.05%\n",
            "\n",
            "Iteration 2900, Val Acc: 81.96%\n",
            "\n",
            "Iteration 3000, Val Acc: 82.68%\n",
            "\n",
            "Iteration 3100, Val Acc: 81.74%\n",
            "\n",
            "Iteration 3200, Val Acc: 81.91%\n",
            "\n",
            "Iteration 3300, Val Acc: 82.14%\n",
            "\n",
            "Iteration 3400, Val Acc: 82.17%\n",
            "\n",
            "Iteration 3500, Val Acc: 82.09%\n",
            "\n",
            "Iteration 3600, Val Acc: 82.87%\n",
            "\n",
            "Iteration 3700, Val Acc: 82.17%\n",
            "\n",
            "Iteration 3800, Val Acc: 81.42%\n",
            "\n",
            "Iteration 3900, Val Acc: 83.09%\n",
            "\n",
            "Iteration 4000, Val Acc: 82.53%\n",
            "\n",
            "Iteration 4100, Val Acc: 82.88%\n",
            "\n",
            "Iteration 4200, Val Acc: 82.71%\n",
            "\n",
            "Iteration 4300, Val Acc: 82.98%\n",
            "\n",
            "Iteration 4400, Val Acc: 83.02%\n",
            "\n",
            "Iteration 4500, Val Acc: 82.90%\n",
            "\n",
            "Iteration 4600, Val Acc: 83.41%\n",
            "\n",
            "Iteration 4700, Val Acc: 82.86%\n",
            "\n",
            "Iteration 4800, Val Acc: 82.30%\n",
            "\n",
            "Iteration 4900, Val Acc: 82.87%\n",
            "\n",
            "Iteration 5000, Val Acc: 82.79%\n",
            "\n",
            "Iteration 5100, Val Acc: 82.85%\n",
            "\n",
            "Iteration 5200, Val Acc: 82.56%\n",
            "\n",
            "Iteration 5300, Val Acc: 82.47%\n",
            "\n",
            "Iteration 5400, Val Acc: 83.12%\n",
            "\n",
            "Iteration 5500, Val Acc: 82.19%\n",
            "\n",
            "Iteration 5600, Val Acc: 83.15%\n",
            "\n",
            "Iteration 5700, Val Acc: 83.10%\n",
            "\n",
            "Iteration 5800, Val Acc: 83.19%\n",
            "\n",
            "Iteration 5900, Val Acc: 82.56%\n",
            "\n",
            "Iteration 6000, Val Acc: 83.43%\n",
            "\n",
            "Iteration 6100, Val Acc: 82.99%\n",
            "\n",
            "Iteration 6200, Val Acc: 82.27%\n",
            "\n",
            "Iteration 6300, Val Acc: 83.50%\n",
            "\n",
            "Iteration 6400, Val Acc: 83.21%\n",
            "\n",
            "Iteration 6500, Val Acc: 83.82%\n",
            "\n",
            "Iteration 6600, Val Acc: 83.34%\n",
            "\n",
            "Iteration 6700, Val Acc: 83.11%\n",
            "\n",
            "Iteration 6800, Val Acc: 83.35%\n",
            "\n",
            "Iteration 6900, Val Acc: 83.46%\n",
            "\n",
            "Iteration 7000, Val Acc: 83.43%\n",
            "\n",
            "Iteration 7100, Val Acc: 83.59%\n",
            "\n",
            "Iteration 7200, Val Acc: 83.21%\n",
            "\n",
            "Iteration 7300, Val Acc: 82.96%\n",
            "\n",
            "Iteration 7400, Val Acc: 82.58%\n",
            "\n",
            "Iteration 7500, Val Acc: 82.79%\n",
            "\n",
            "Iteration 7600, Val Acc: 83.10%\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a6bd57225ee7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mspk_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspike_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# initialize the loss & sum over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f846113e671f>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(net, num_steps, spike_data)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mspk_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0mspk_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mmem_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
        "num_epochs = 40\n",
        "loss_hist = []\n",
        "val_acc_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #Inner training loop\n",
        "    for data, targets in iter(train_loader):\n",
        "        spike_data = spikegen.rate(data, num_steps=num_steps)\n",
        "        spike_data = spike_data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        net.train()\n",
        "        spk_rec, _ = forward_pass(net, num_steps, spike_data)\n",
        "\n",
        "        # initialize the loss & sum over time\n",
        "        loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Validation set\n",
        "        if counter % 100 == 0:\n",
        "          with torch.no_grad():\n",
        "              net.eval()\n",
        "\n",
        "              # Validation set forward pass\n",
        "              val_acc = batch_accuracy(val_loader, net, num_steps)\n",
        "              print(f\"Iteration {counter}, Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "              val_acc_hist.append(val_acc.item())\n",
        "\n",
        "        counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DePd5NLivojk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c795f0b-7a50-4c74-d76f-62a04de4976c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 83.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test accuracy\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    # Test set forward pass\n",
        "    test_acc = batch_accuracy(test_loader, net, num_steps)\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}